#!/usr/bin/env python3
"""
Generate crawls.mk with targets for each Common Crawl collection.
Downloads the index page and creates makefile targets.
"""

import re
import urllib.request
import sys

def fetch_crawl_collections():
    """Fetch available crawl collections from Common Crawl index."""
    index_url = "https://data.commoncrawl.org/cc-index/collections/index.html"
    
    try:
        with urllib.request.urlopen(index_url) as response:
            html = response.read().decode('utf-8')
    except Exception as e:
        print(f"Error fetching {index_url}: {e}", file=sys.stderr)
        sys.exit(1)
    
    # Extract collection names like CC-MAIN-2024-10
    collections = re.findall(r'CC-MAIN-\d{4}-\d{2}', html)
    
    if not collections:
        print("No collections found in index page", file=sys.stderr)
        sys.exit(1)
        
    return sorted(set(collections))

def generate_makefile(collections):
    """Generate makefile content for all collections."""
    print("# Generated by generate_crawls_mk.py")
    print("# Common Crawl domain extraction targets")
    print()
    print(".PHONY: all clean stats")
    print()
    
    # All target
    all_targets = " ".join([f"{col}.tsv.gz" for col in collections])
    print(f"all: {all_targets}")
    print()
    
    # Individual collection targets
    for collection in collections:
        tsv_file = f"{collection}.tsv"
        gz_file = f"{tsv_file}.gz"
        paths_url = f"https://data.commoncrawl.org/cc-index/collections/{collection}/indexes.paths.gz"
        
        print(f"# {collection}")
        print(f"{gz_file}: {tsv_file}")
        print(f"\tgzip -9 $<")
        print()
        
        print(f"{tsv_file}:")
        print(f"\t./extract_domains.py '{paths_url}' > $@")
        print()
    
    # Clean target
    print("clean:")
    print("\trm -f *.tsv *.tsv.gz")
    print()
    
    # Stats target
    print("stats:")
    print("\t@echo 'Collection statistics:'")
    for collection in collections:
        gz_file = f"{collection}.tsv.gz"
        print(f"\t@if [ -f {gz_file} ]; then echo '{collection}: '`zcat {gz_file} | wc -l`' domains'; fi")

def main():
    collections = fetch_crawl_collections()
    generate_makefile(collections)

if __name__ == "__main__":
    main()