#!/usr/bin/env python3
"""
Generate crawls.mk with targets for each Common Crawl collection.
Downloads the index page and creates makefile targets.
"""

import re
import urllib.request
import sys

def fetch_index_paths():
    """Fetch available index paths from Common Crawl index."""
    index_url = "https://data.commoncrawl.org/cc-index/collections/index.html"
    
    try:
        with urllib.request.urlopen(index_url) as response:
            html = response.read().decode('utf-8')
    except Exception as e:
        print(f"Error fetching {index_url}: {e}", file=sys.stderr)
        sys.exit(1)
    
    # Extract href and link text for cc-index.paths.gz files
    matches = re.findall(r'href="(/[^"]+/cc-index\.paths\.gz)">([^/<]+)/', html)
    
    if not matches:
        print("No index paths found in index page", file=sys.stderr)
        sys.exit(1)
        
    return sorted(matches)

def path_to_name(path):
    """Convert a path to a collection name."""
    if 'CC-MAIN-' in path:
        return re.search(r'(CC-MAIN-\d{4}-\d{2})', path).group(1)
    elif 'crawl-001' in path:
        return 'crawl-001'
    elif 'crawl-002' in path:
        return 'crawl-002' 
    elif 'parse-output' in path:
        return 'parse-output'
    else:
        return path.replace('/', '_').replace('.gz', '')

def generate_makefile(matches):
    """Generate makefile content for all index paths."""
    print("# Generated by prep_make.py")
    print("# Common Crawl domain extraction targets")
    print()
    print(".PHONY: all clean-crawls stats")
    print()
    
    # Create name->url mapping from (path, name) tuples
    collections = []
    for path, name in matches:
        # Clean up the name (remove extra whitespace, make filesystem-safe)
        name = re.sub(r'\s+', '-', name.strip())
        name = re.sub(r'[^\w\-.]', '', name)
        full_url = f"https://data.commoncrawl.org{path}"
        collections.append((name, full_url))
    
    # All target (reversed to process newest first)
    all_targets = " ".join([f"{name}.tsv.gz" for name, _ in reversed(collections)])
    print(f"all: {all_targets}")
    print()
    
    # Individual collection targets (just .tsv, .gz handled by pattern rule)
    for name, url in collections:
        tsv_file = f"{name}.tsv"
        
        print(f"# {name}")
        print(f"{tsv_file}: cdx")
        print(f"\t./crawl.sh '{url}' > $@")
        print()
    
    # Clean target (avoid conflicts with main Makefile)
    print("clean-crawls:")
    print("\trm -f *.tsv *.tsv.gz")
    print()
    
    # Stats target
    print("stats:")
    print("\t@echo 'Collection statistics:'")
    for name, _ in collections:
        gz_file = f"{name}.tsv.gz"
        print(f"\t@if [ -f {gz_file} ]; then echo '{name}: '`zcat {gz_file} | wc -l`' domains'; fi")

def main():
    matches = fetch_index_paths()
    generate_makefile(matches)

if __name__ == "__main__":
    main()